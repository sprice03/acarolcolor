---
title: "Temperature Data Prep"
format: html
editor: visual
---

Load data

```{r}

# load from csv in "data" folder
data <- finaldata
```

Download PRISM dailies as required depending on what years are needed

```{r}

library(prism)

#get_prism_dailys(
#  type="tmin", # need to do this for both tmin and tmax
#  minDate = "2023-01-01", # change these min and max dates
#  maxDate = "2024-01-01"
#)
```

Bind grid cells to data

```{r}

# df must have:
# 1. latitude and longitude cols
bindGridCells <- function(df,cellsize_km=25){
    library(sf)
    library(dplyr)

    df$lat_temp <- df$latitude
    df$lon_temp <- df$longitude

    # Convert df to sf object
    sf_data <- st_as_sf(df, coords = c("longitude", "latitude"), crs = 4326)

    # Create the grid
    grid <- st_make_grid(sf_data, cellsize = c(cellsize_km * 0.009, cellsize_km * 0.009), square = TRUE) %>%
        st_sf('ID' = 1:length(.), geometry = .)

    # Calculate the center (centroid) of each grid cell
    grid_centers <- st_centroid(grid)

    # Add centroids' coordinates to the grid data
    grid_with_centroids <- cbind(grid, st_coordinates(grid_centers))

    # Perform spatial join, joining sf_data with the nearest grid cell
    result <- st_join(sf_data, grid, left = TRUE)

    # Add the cell column based on the grid ID
    result$cell <- result$ID

    # Add the latitude and longitude of grid cell centers to result
    result <- result %>%
        mutate(
            cell_lat = grid_with_centroids$Y[result$ID],
            cell_lon = grid_with_centroids$X[result$ID]
        )

    result$latitude <- result$lat_temp
    result$longitude <- result$lon_temp

    return(data.frame(result))
}

bindSeasonIntervals <- function(df, season_intervals_per_year=4){
    library(lubridate)
    df$yday <- yday(df$datetime)
    # 4 season intervals
    if(season_intervals_per_year == 4){
        df$season <- "1"
        df$season[df$yday >= 61 & df$yday < 153] <- "2"
        df$season[df$yday >= 153 & df$yday < 245] <- "3"
        df$season[df$yday >= 245 & df$yday < 336] <- "4"
    }

    # 8 season intervals
    if(season_intervals_per_year == 8){
        df$season <- "1"
        df$season[df$yday >= 13 & df$yday < 59] <- "2"
        df$season[df$yday >= 59 & df$yday < 106] <- "3"
        df$season[df$yday >= 106 & df$yday < 153] <- "4"
        df$season[df$yday >= 153 & df$yday < 200] <- "5"
        df$season[df$yday >= 200 & df$yday < 247] <- "6"
        df$season[df$yday >= 247 & df$yday < 294] <- "7"
        df$season[df$yday >= 294 & df$yday < 340] <- "8"
    }
    return(df)
}

bindDayMonthYearSeason <- function(d) {
    d <- bindSeasonIntervals(d,season_intervals_per_year = 4)

    # Extracting month and year from a date column (assuming the date column is named 'date')
    d <- d %>%
        mutate(
            month = lubridate::month(datetime),
            year = lubridate::year(datetime),
        )

    return(d)
}
data$latitude <- data$latitude.x
data$longitude <- data$longitude.x

d <- bindGridCells(data,cellsize_km = 250)
d <- bindDayMonthYearSeason(d)
d <- dplyr::filter(d, year == "2023") # change this to be all years you want to use, use just 2023 for testing
```

Bind Prism data

```{r}

bindPrismData <- function(df, prism_dir, cell_width_km = 25, variable = "tmin", res = "daily", new_var_colname = "prism_tmin", avg_of_years = NULL) {
    library(dplyr)
    library(geosphere)
    library(raster)
    library(sf)
    library(rgdal)
    library(prism)

    # Set the PRISM download directory
    prism_set_dl_dir(prism_dir)

    # Filter data to keep only points within the contiguous USA
    df <- df %>%
        filter(cell_lat >= 24.52, cell_lat <= 49.38,  # Latitude of the contiguous USA
               cell_lon >= -125.0, cell_lon <= -66.94)

    # Select relevant columns based on the resolution type
    if (res == "daily") {
        df2 <- df %>% dplyr::select(datetime, cell, cell_lat, cell_lon)
        df2 <- df2 %>%
            mutate(date = as.Date(datetime)) %>%
            distinct(cell, date, .keep_all = TRUE)
    } else if (res == "monthly") {
        df2 <- df %>% dplyr::select(year, month, cell, cell_lat, cell_lon)
        df2 <- df2 %>%
            distinct(cell, year, month, .keep_all = TRUE)
    } else if (res == "season") {
        df2 <- df %>% dplyr::select(year, season, cell, cell_lat, cell_lon)
        df2 <- df2 %>%
            distinct(cell, year, season, .keep_all = TRUE)
    }

    # Filter out rows with missing latitude or longitude
    df2 <- df2 %>% filter(!is.na(cell_lat) & !is.na(cell_lon))

    # Generate polygons for unique cell locations
    df2 <- df2 %>%
        rowwise() %>%
        mutate(cell_poly = list(cellPolyFromLatLonWidth(cell_lat, cell_lon, cell_width_km)))

    if (!is.null(avg_of_years)) {
        df2 <- df2 %>%
            distinct(cell, across(-year), .keep_all = TRUE)
    }

    # Helper function for daily data extraction
    getDailyPrismData <- function(date, cell_poly, variable) {
        pd <- prism_archive_subset(variable, "daily", dates = date)
        if (length(pd) == 0) return(NA)
        file <- pd_to_file(pd)[1]
        raster_data <- raster(file)
        r_crop <- crop(raster_data, extent(cell_poly))
        r_mask <- mask(r_crop, cell_poly)
        avg_value <- cellStats(r_mask, stat = 'mean')
        return(avg_value)
    }

    # Helper function for monthly data extraction
    getMonthlyPrismData <- function(year, month, cell_poly, variable) {
        pd <- prism_archive_subset(variable, "monthly", mon = month, years = year)
        if (length(pd) == 0) return(NA)
        file <- pd_to_file(pd)[1]
        raster_data <- raster(file)
        r_crop <- crop(raster_data, extent(cell_poly))
        r_mask <- mask(r_crop, cell_poly)
        avg_value <- cellStats(r_mask, stat = 'mean')
        return(avg_value)
    }

    # Helper function for seasonal data extraction
    getYearSeasonPrismData <- function(year, season, cell_poly, variable) {
        mons <- switch(season,
                       "1" = c(12, 1, 2),  # Winter
                       "2" = c(3, 4, 5),   # Spring
                       "3" = c(6, 7, 8),   # Summer
                       "4" = c(9, 10, 11)) # Fall
        pd <- prism_archive_subset(variable, "monthly", mon = mons, years = year)
        if (length(pd) < 3) return(NA)  # Ensure all 3 months are available
        files <- pd_to_file(pd)
        raster_avg <- calc(stack(lapply(files, raster)), fun = mean)
        r_crop <- crop(raster_avg, extent(cell_poly))
        r_mask <- mask(r_crop, cell_poly)
        avg_value <- cellStats(r_mask, stat = 'mean')
        return(avg_value)
    }

    # Initialize an empty column for storing values
    df2$value <- NA
    polys <- as.list(df2$cell_poly)

    print(paste0("Number of cell combinations to get Prism data for: ", length(polys)))

    # Loop to process each unique cell combination and fetch data
    for (i in seq_along(polys)) {
        print(paste("Processing:", i))
        poly <- polys[[i]]
        if (res == "daily") {
            date <- df2$date[i]
            df2$value[i] <- getDailyPrismData(date, poly, variable)
        } else if (res == "monthly") {
            year <- df2$year[i]
            month <- df2$month[i]
            df2$value[i] <- getMonthlyPrismData(year, month, poly, variable)
        } else if (res == "season") {
            year <- df2$year[i]
            season <- df2$season[i]
            df2$value[i] <- getYearSeasonPrismData(year, season, poly, variable)
        }
    }

    # Bind the calculated data back to the original dataframe
    bound <- df %>%
        left_join(df2 %>% dplyr::select(-cell_poly), by = intersect(names(df), names(df2)))

    # Rename the new column
    bound <- bound %>% rename(!!new_var_colname := value)
    return(bound)
}

data <- bindPrismData(d,prism_dir="prism_dailies", variable="tmin", new_var_colname = "prism_tmin") #change prism dir to its location
data <- bindPrismData(d,prism_dir="prism_dailies", variable="tmax", new_var_colname = "prism_tmax")

```

Add date, time, and latitude columns we need to infer hourly temperatures

```{r}

# filter out UTC datetimes so we keep local only
data <- data[!str_detect(as.character(data$datetime), "UTC"), ]

# add month, day, latitude, and local hour columns
dates <- as.Date(data$yday - 1, origin = "2021-01-01")
data$Month <- month(dates)
data$Day <- day(dates)
data$Latitude <- round(data$latitude)
data$local_hour <- hour(ymd_hms(data$datetime, tz = ""))
```

Infer hourly temperatures, now row by row rather than lat by lat

```{r}

data_for_hourly <- data %>% dplyr::select(c(year,tmin,tmax,Month,Day,Latitude,id,local_hour))

# Initialize an empty dataframe to store results
hourly_temp_data <- data.frame()

# now one row at a time for simplicity
for (i in seq_len(nrow(data_for_hourly))) {
    # Extract the current row
    row_data <- data_for_hourly[i, ]
    
    # Prepare the input for stack_hourly_temps (convert to dataframe)
    temp_input <- data.frame(
        Year = row_data$year,
        Tmin = row_data$tmin,
        Tmax = row_data$tmax,
        Month = row_data$Month,
        Day = row_data$Day
    )
    
    # need input to be at least two rows or it bugs
    temp_input <- rbind(temp_input,temp_input)
    
    # Process hourly temperatures for the row
    hourtemps <- stack_hourly_temps(temp_input, latitude = row_data$Latitude)$hourtemps

    temp <- hourtemps %>% filter(Hour==row_data$local_hour)
    temp <- temp$Temp[1]
    
    row_data$hour_temperature <- temp
    
    # Append the result
    hourly_temp_data <- rbind(hourly_temp_data, row_data)
}

# this is the final data
data <- hourly_temp_data
```

Examine the final data

```{r}

View(data)
```
